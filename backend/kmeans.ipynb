{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates what runs on the backend. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE--color quantization performed according to scikit-learn docs: https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read image to a numpy array\n",
    "We use PIL, the Python Imaging Library, in order to read the image from either a filepath (or from a request in the backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pandas as pd\n",
    "\n",
    "size = 100\n",
    "im = Image.open(\"images/sunflower.jpg\") # create PIL image\n",
    "im.thumbnail([size,size], Image.LANCZOS)\n",
    "imageArray = np.asarray(im) # read into numpy array\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train image on Mini-batch K-means\n",
    "\n",
    "We use mini batch K-means as it's faster and we're working with relatively small amounts of data. This will ensure the model runs quicker on the backend and serve a result to the user quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, d = tuple(imageArray.shape) # get width and height of image\n",
    "\n",
    "n_colors = 10 # number of colors to use\n",
    "\n",
    "flattenedImageArray = np.reshape(imageArray, (w * h, d)) # flatten image array for training\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters = n_colors, n_init = 'auto', random_state = 0).fit( # train kmeans model on flattened array, n_init = auto means kmeans\n",
    "    flattenedImageArray                                                                 # will have one run with centroid sampled based on contribution to \n",
    ")                                                                                       # inertia and not just randomly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels for original image, this will generate a 1-d array that corresponds \n",
    "# to the original flattened array where each element corresponds to an index into the codebook (i.e. kmeans.cluster_centers_)\n",
    "labels = kmeans.predict(flattenedImageArray) \n",
    "\n",
    "def convertPaletteToDMC(palette, dmc_url):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "    palette: array-like of cluster centers\n",
    "    dmc_url: url containing dataset of floss colors\n",
    "    PURPOSE:\n",
    "    This function accepts a palette (codebook) and converts it into \n",
    "    RGB colors that represent DMC floss.\n",
    "    \"\"\"\n",
    "    dmc_df = pd.read_csv(dmc_url, header = None)\n",
    "    return np.asarray([resolvePixel(pix, dmc_df) for pix in palette])\n",
    "    \n",
    "def resolvePixel(pixel, df):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "    pixel: array of length 3 containing rgb values for a pixel you want to map to a DMC color\n",
    "    df: pandas DataFrame containing DMC floss and their RGB values. expects columns 2-4 to be RGB, last col to be color description\n",
    "    PURPOSE:\n",
    "    Computes which dmc color is the closest in terms of euclidean distance (plot rgb values on 3d grid, pick the closest)\n",
    "    \"\"\"\n",
    "    bestDist = 99999 # initialize distance to some large number\n",
    "    bestColor = -1   # initialize color to some dummy value\n",
    "    # bestDescription = \"\" # in case you want to use description\n",
    "    for color in df.iterrows():\n",
    "        dist = np.sqrt(np.sum((pixel-color[1][1:-1])**2)) # compute euclidean distance between DMC color and pixel\n",
    "        if dist < bestDist: # if it does better, update\n",
    "            bestDist = dist\n",
    "            bestColor = color[1][1:-1]\n",
    "            # bestDescription = color[1][4]\n",
    "    return np.asarray(bestColor) # return as numpy array\n",
    "    \n",
    "\n",
    "def gen_image(palette, labels, w, h):\n",
    "    return palette[labels].reshape(w, h, -1) # -1 means infer third dimension (should be 3)\n",
    "\n",
    "palette = convertPaletteToDMC(kmeans.cluster_centers_, \"https://storage.googleapis.com/dmc-cathacks/dmc.csv\")\n",
    "finalImageArray = gen_image(palette, labels, w, h).astype(np.uint8) # convert floats to uint8 for PIL\n",
    "finalImage = Image.fromarray(finalImageArray) # create PIL image from output array\n",
    "\n",
    "# plt.imshow(finalImage) # uncomment to show result, hidden to not distort GitHub language percentages\n",
    "\n",
    "                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
